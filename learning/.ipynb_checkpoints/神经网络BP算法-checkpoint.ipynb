{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " ###  BP算法\n",
    "\n",
    "　　输入：训练集D=(xk,yk)mk=1; 学习率;\n",
    "\n",
    "　　过程：\n",
    "\n",
    "　　1. 在(0, 1)范围内随机初始化网络中所有连接权和阈值\n",
    "\n",
    "　　2. repeat:\n",
    "\n",
    "　　3.　　 for all ($x_k$, $y_k$)∈D do\n",
    "\n",
    "　　4. 　　　　根据当前参数计算当前样本的输出$y_{outpout}{k} = f(\\beta_j-\\theta_j)$; f为输出函数，一般为sigmoid函数\n",
    "\n",
    "　　5. 　　　　计算输出层神经元的梯度项$ g_j = y_{outpout}{k}*(1-y_{outpout}{k})(y_j^k-y_{outpout}{k})$；\n",
    "\n",
    "　　6. 　　　　计算隐层神经元的梯度项$e_h = $；\n",
    "\n",
    "　　7. 　　　　更新连接权与阈值\n",
    "\n",
    "　　8. 　　end for\n",
    "\n",
    "　　9. until 达到停止条件\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 公式\n",
    "第j个神经元的输出：$y_j^k$ <br>\n",
    "$y_j^k = f(\\beta_j - \\theta_j)$  <br>\n",
    "\n",
    "输出层到隐藏层的参数偏导：$g_i$  <br>\n",
    "$g_i = \\frac{\\partial E_k}{\\partial y_{outputj}^k}* \\frac{\\partial y_{outputj}^k}{\\partial \\beta_j}\n",
    "     =y_{outputj}^k *(1-y_{outputj}^k)(y_j^k - y_{outputj}^k) $ <br>\n",
    "\n",
    "隐藏层到输入层的参数偏导: $e_h$ <br>\n",
    "$e_h = -\\frac{\\partial E_k}{\\partial b_h}*\\frac{\\partial b_h}{\\partial \\alpha_h}\n",
    "     =b_h*(1-b_h)\\sum_{j=1}^l\\omega_{hj}g_j$   <br>\n",
    "\n",
    "重要的参数更新： <br>    \n",
    "$\\Delta\\omega_{hj} = \\eta*g_j*b_h$  <br>\n",
    "$\\Delta\\theta_j = - \\eta g_j$       <br>\n",
    "$\\Delta v_{ih} = \\eta*e_hx_i$       <br>\n",
    "$\\Delta\\gamma_h = -\\eta*e_h$        <br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新偏导参数, 矩阵形式\n",
    "def Gi(y, y_output):\n",
    "    return y_output*(1-y_output)*(y-y_output)\n",
    "def Eh(b, w_h, g):\n",
    "    return b*(1-b)*np.dot(w_h,g)\n",
    "def Delta_omega_all(eta, g, b):\n",
    "#     eta是学习率\n",
    "#   参数omega是连接隐藏层到输出层的参数 omega=(len(g), len(b))\n",
    "    print(\"g的shape \", g.shape, \"b的shape \",b.shape)\n",
    "    return eta*np.dot(g, b)\n",
    "\n",
    "def Detal_theta_all(eta, g):\n",
    "    return eta*g\n",
    "\n",
    "def Delta_V_all(eta, Eh, X):\n",
    "#   参数V是连接输入层到隐藏层的 V=(len(Eh), len(X))\n",
    "    return eta*np.dot(Eh, X)\n",
    "def Delta_gamme(eta, Eh):\n",
    "    return -eta*Eh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,) (2,)\n",
      "(3, 1) (1, 2)\n",
      "[[ 0  0]\n",
      " [ 4  5]\n",
      " [ 8 10]]\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(0,3).reshape(len(a), 1)  \n",
    "b = np.arange(4,6)\n",
    "print(b.shape, np.transpose(b).shape)\n",
    "b = b.reshape(1, len(b))\n",
    "print(a.shape, b.shape)\n",
    "print(np.dot(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 48,  80, 120])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gi(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
