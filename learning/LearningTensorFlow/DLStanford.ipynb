{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#  TF程序的两步： ①自己构建计算图   ②利用tf.Session 执行计算图中的计算流程\n",
    "#  或者利用default_graph\n",
    "#  一个小trick  小写的代表op  大写的代表class 一个class包含多个op\n",
    "g1 = tf.Graph()\n",
    "g2 = tf.get_default_graph()\n",
    "\n",
    "with g1.as_default():\n",
    "    a = tf.constant(3)\n",
    "    b = tf.constant(4)\n",
    "    c = tf.add(a, b)\n",
    "with g2.as_default():\n",
    "    d = tf.constant(4)\n",
    "    e = tf.constant(4)\n",
    "    f = tf.add(d, e)\n",
    "\n",
    "with tf.Session(graph=g1) as sess:\n",
    "    print(sess.run(a))\n",
    "    \n",
    "with tf.Session(graph=g2) as sess:\n",
    "    print(sess.run(f))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(2)\n",
    "b = tf.constant(3)\n",
    "c = tf.add(a,b)\n",
    "\n",
    "writer = tf.summary.FileWriter('./graphs', tf.get_default_graph())\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "100\n",
      "100\n",
      "None\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "#   通过get_variable 方法创建变量， 也可以直接通过\n",
    "# W = tf.get_variable('big_matrixx', shape=(784, 10), initializer=tf.zeros_initializer())\n",
    "\n",
    "#   初始化变量\n",
    "# with tf.Session() as sess:\n",
    "#     print(sess.run(W.initializer))\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     sess.run(W)\n",
    "\n",
    "w = tf.Variable(10)\n",
    "w.assign(20)  # assign 复制方法会产生一个新的Tensor\n",
    "# 下面将变量w的操作存储\n",
    "#  每个Session 有自己的变量空间，以及这些变量对应的操作op\n",
    "w_assign_op =  w.assign(100)\n",
    "w_assign_op_double = w.assign(2*w)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(w.initializer)\n",
    "#     sess.run(w_assign_op)\n",
    "    sess.run(w_assign_op_double)\n",
    "    sess.run(w_assign_op_double)\n",
    "    print(w.eval())\n",
    "    print(w_assign_op.eval())\n",
    "    print(w.eval())\n",
    "\n",
    "sess2 = tf.Session()\n",
    "sess2.run(w.initializer)\n",
    "print(sess2.run(w.assign_add(2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.  7.  8.]\n",
      "[ 6.  6.  6.]\n"
     ]
    }
   ],
   "source": [
    "#   利用placeholder来存储变量\n",
    "#   使用placeholder的变量的时候，要使用feed_dict 给变量复制\n",
    "a = tf.placeholder(tf.float32, shape=[3])\n",
    "b = tf.constant([5,5,5], tf.float32)\n",
    "\n",
    "c = a + b\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c, feed_dict={a:[1,2,3]}))\n",
    "    print(sess.run(c, feed_dict={a:[1,1,1]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package tensorflow.data in tensorflow:\n",
      "\n",
      "NAME\n",
      "    tensorflow.data - `tf.data.Dataset` API for input pipelines.\n",
      "\n",
      "DESCRIPTION\n",
      "    See @{$guide/datasets$Importing Data} for an overview.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "\n",
      "\n",
      "FILE\n",
      "    c:\\users\\ibm\\appdata\\roaming\\python\\python35\\site-packages\\tensorflow\\data\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### TF control flow\n",
    "`control flow ops`: tf.group, tf.count_up_to, tf.cond, tf.case, tf.while_llop, .... <br>\n",
    "`comparsion ops`:   tf.equal, tf.not_equal, tf.less, tf.greater, tf.where, ...      <br>\n",
    "`logical ops`:      tf.logical_and, tf.logical_not, tf.logical_or, tf.logical_xor...<br>\n",
    "`debugging ops`:    tf.is_finite, tf.is_inf, tf.is_nan, tf.Assert, tf.Print ....    <br> \n",
    "\n",
    "#### placeholder\n",
    "优点： 容易操作，方便处理\n",
    "缺点： 容易造成性能瓶颈\n",
    "\n",
    "####   利用tf.data.DataSet 来加载和存储数据\n",
    "tf.data 和 feed_dict 的比较： 利用dict来feed更加简单（python的形式），当数据量不大的时候， 建议采用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[768.0, 32.0]\n"
     ]
    }
   ],
   "source": [
    "#  变量共享的问题 problem when Variable sharing\n",
    "# def two_hidden_layers(x):\n",
    "#     w1 = tf.Variable(tf.random_normal([100, 50]), name='h1_weights')\n",
    "#     b1 = tf.Variable(tf.zeros([50]), name='h1_biases')\n",
    "#     h1 = tf.matmul(x, w1) + b1\n",
    "    \n",
    "#     w2 = tf.Variable(tf.random_normal([50, 10]), name='h2_weights')\n",
    "#     b2 = tf.Variable(tf.zeros([10]), name='h2_biases')\n",
    "#     logits = tf.matmul(h1, w2) + b2\n",
    "#     return logits\n",
    "x1 = tf.random_normal([50, 100])\n",
    "x2 = tf.random_normal([50, 100])\n",
    "# logits1 = two_hidden_layers(x1)\n",
    "# logits2 = two_hidden_layers(x2)\n",
    "\n",
    "# print(logits1[0][0].get, logits2[0][0])\n",
    "\n",
    "# 通过get_variable 来共享变量\n",
    "# def two_hidden_layers_share(x):\n",
    "#     assert x.shape.as_list() == [200, 100]\n",
    "#     w1 = tf.get_variable('h1_weights', [100, 50], initializer=tf.random_normal_initializer())\n",
    "#     b1 = tf.get_variable('h1_biases', [50], initializer=tf.constant_initializer(0.0))\n",
    "#     h1 = tf.matmul(x, w1) + b1\n",
    "     \n",
    "#     assert h1.shape.as_list() == [200, 50]\n",
    "#     w2 = tf.get_variable('h2_weights', [50, 10], initializer=tf.random_normal_initializer())\n",
    "#     b2 = tf.get_variable('b2_biases', [10], initializer=tf.constant_initializer(0.0))\n",
    "#     logits = tf.matmul(h1, w2) + b2\n",
    "#     return logits\n",
    "#  如果直接这么共享变量的话，会出现ValueError:Variable h1_weights already exists 之类的错误\n",
    "# logits1 = two_hidden_layers_share(x1)\n",
    "# logits2 = two_hidden_layers_share(x2)\n",
    "\n",
    "# 如下，将共享的变量放入一个scope当中，在这个scope中复用这些变量\n",
    "# with tf.variable_scope('two_layers', reuse=tf.AUTO_REUSE) as scope:\n",
    "#     logits1 = two_hidden_layers_share(x1)\n",
    "#     scope.reuse_variables()\n",
    "#     logits2 = two_hidden_layers_share(x2)\n",
    "#     print(logits1[0][0], logits2[0][0])\n",
    "   \n",
    "\n",
    "\n",
    "# 通过在函数中定义 variable_scope 来定义不同的变量空间  来更好的复用变量\n",
    "# def fully_connected(x, output_dim, scope):\n",
    "#     with tf.variable_scope(scope, reuse=tf.AUTO_REUSE) as scope:\n",
    "#         w = tf.get_variable('weights', [x.shape[1], output_dim], initializer=tf.random_normal_initializer())\n",
    "#         b = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\n",
    "#         return tf.matmul(x, w) + b\n",
    "\n",
    "# def two_hidden_layers(x):\n",
    "#     h1 = fully_connected(x, 50, 'h1')\n",
    "#     h2 = fully_connected(h1, 10, 'h2')\n",
    "\n",
    "# with tf.variable_scope('two_layers') as scope:\n",
    "#     logits1 = two_hidden_layers(x1)\n",
    "#     logits2 = two_hidden_layers(x2)\n",
    "    \n",
    "# tf.train.Saver 保存和存储变量\n",
    "# 保存最近的latest检查点\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 随机性random  根据random的seed不同可以产生不同的随机值\n",
    "# def produceRandom(seedNumber):\n",
    "#     return tf.random_uniform([], -10, 10, seed=seedNumber)\n",
    "\n",
    "# c = tf.random_uniform([], -10, 10, seed=2)\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     print(sess.run(produceRandom(2)))\n",
    "#     print(sess.run(produceRandom(3)))\n",
    "    \n",
    "# with tf.Session() as sess:\n",
    "#     print(sess.run(c))\n",
    "# with tf.Session() as sess:\n",
    "#     print(sess.run(c))    \n",
    "\n",
    "\n",
    "# 利用 tf.gradients() 计算偏导\n",
    "x = tf.Variable(2.0)\n",
    "y = 2.0 * (x**3)\n",
    "z = 3.0 + y ** 2\n",
    "grad_z = tf.gradients(z, [x, y])\n",
    "with tf.Session()  as sess:\n",
    "    sess.run(x.initializer)\n",
    "    print(sess.run(grad_z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function gradients in module tensorflow.python.ops.gradients_impl:\n",
      "\n",
      "gradients(ys, xs, grad_ys=None, name='gradients', colocate_gradients_with_ops=False, gate_gradients=False, aggregation_method=None, stop_gradients=None)\n",
      "    Constructs symbolic derivatives of sum of `ys` w.r.t. x in `xs`.\n",
      "    \n",
      "    `ys` and `xs` are each a `Tensor` or a list of tensors.  `grad_ys`\n",
      "    is a list of `Tensor`, holding the gradients received by the\n",
      "    `ys`. The list must be the same length as `ys`.\n",
      "    \n",
      "    `gradients()` adds ops to the graph to output the derivatives of `ys` with\n",
      "    respect to `xs`.  It returns a list of `Tensor` of length `len(xs)` where\n",
      "    each tensor is the `sum(dy/dx)` for y in `ys`.\n",
      "    \n",
      "    `grad_ys` is a list of tensors of the same length as `ys` that holds\n",
      "    the initial gradients for each y in `ys`.  When `grad_ys` is None,\n",
      "    we fill in a tensor of '1's of the shape of y for each y in `ys`.  A\n",
      "    user can provide their own initial `grad_ys` to compute the\n",
      "    derivatives using a different initial gradient for each y (e.g., if\n",
      "    one wanted to weight the gradient differently for each value in\n",
      "    each y).\n",
      "    \n",
      "    `stop_gradients` is a `Tensor` or a list of tensors to be considered constant\n",
      "    with respect to all `xs`. These tensors will not be backpropagated through,\n",
      "    as though they had been explicitly disconnected using `stop_gradient`.  Among\n",
      "    other things, this allows computation of partial derivatives as opposed to\n",
      "    total derivatives. For example:\n",
      "    \n",
      "    ```python\n",
      "    a = tf.constant(0.)\n",
      "    b = 2 * a\n",
      "    g = tf.gradients(a + b, [a, b], stop_gradients=[a, b])\n",
      "    ```\n",
      "    \n",
      "    Here the partial derivatives `g` evaluate to `[1.0, 1.0]`, compared to the\n",
      "    total derivatives `tf.gradients(a + b, [a, b])`, which take into account the\n",
      "    influence of `a` on `b` and evaluate to `[3.0, 1.0]`.  Note that the above is\n",
      "    equivalent to:\n",
      "    \n",
      "    ```python\n",
      "    a = tf.stop_gradient(tf.constant(0.))\n",
      "    b = tf.stop_gradient(2 * a)\n",
      "    g = tf.gradients(a + b, [a, b])\n",
      "    ```\n",
      "    \n",
      "    `stop_gradients` provides a way of stopping gradient after the graph has\n",
      "    already been constructed, as compared to `tf.stop_gradient` which is used\n",
      "    during graph construction.  When the two approaches are combined,\n",
      "    backpropagation stops at both `tf.stop_gradient` nodes and nodes in\n",
      "    `stop_gradients`, whichever is encountered first.\n",
      "    \n",
      "    All integer tensors are considered constant with respect to all `xs`, as if\n",
      "    they were included in `stop_gradients`.\n",
      "    \n",
      "    Args:\n",
      "      ys: A `Tensor` or list of tensors to be differentiated.\n",
      "      xs: A `Tensor` or list of tensors to be used for differentiation.\n",
      "      grad_ys: Optional. A `Tensor` or list of tensors the same size as\n",
      "        `ys` and holding the gradients computed for each y in `ys`.\n",
      "      name: Optional name to use for grouping all the gradient ops together.\n",
      "        defaults to 'gradients'.\n",
      "      colocate_gradients_with_ops: If True, try colocating gradients with\n",
      "        the corresponding op.\n",
      "      gate_gradients: If True, add a tuple around the gradients returned\n",
      "        for an operations.  This avoids some race conditions.\n",
      "      aggregation_method: Specifies the method used to combine gradient terms.\n",
      "        Accepted values are constants defined in the class `AggregationMethod`.\n",
      "      stop_gradients: Optional. A `Tensor` or list of tensors not to differentiate\n",
      "        through.\n",
      "    \n",
      "    Returns:\n",
      "      A list of `sum(dy/dx)` for each x in `xs`.\n",
      "    \n",
      "    Raises:\n",
      "      LookupError: if one of the operations between `x` and `y` does not\n",
      "        have a registered gradient function.\n",
      "      ValueError: if the arguments are invalid.\n",
      "      RuntimeError: if called in Eager mode.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
